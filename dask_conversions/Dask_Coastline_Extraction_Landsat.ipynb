{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic + imports likely common across all notebooks\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "# Supress Warning \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Set reference for util modules\n",
    "import sys\n",
    "sys.path.append('/home/jovyan/odc-hub/')\n",
    "# Generic python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr \n",
    "\n",
    "# Bonus vector manipulation\n",
    "import pandas as pd\n",
    "#from pandas import Dataframe\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "from datetime import datetime\n",
    "\n",
    "CMAP = \"Blues\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coastal Erosion Notebook\n",
    "\n",
    "# Under Development\n",
    "\n",
    "## Summary\n",
    "This notebook takes advantage of a time series of Landsat-8 to produce an annual coastline product. \n",
    "\n",
    "## Overview\n",
    "This product is produced, by loading in Landsat-8 data on an annual basis. For each scene a wofs product is produced to identify pixels containing water, as well as cloud and shadows masked. (This step could be replaced in further iterations by using pre-loaded wofs products to increase the computatoinal efficiency). Each scene is matched with it's equivalent tidal height and filtered images within a certain tidal range. Summary annual products are then produced, then shorelines extracted. \n",
    "\n",
    "This notebook takes advantage of DASK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/SatelliteApplicationsCatapult/datacube-utilities.git#egg=datacube_utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required DC utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required datacube modules\n",
    "from datacube_utilities.interactive_maps import display_map\n",
    "from datacube_utilities.clean_mask import landsat_qa_clean_mask\n",
    "import datacube_utilities.waterline_functions_deaafrica as waterline_funcs\n",
    "from datacube_utilities.dc_water_classifier import wofs_classify\n",
    "from datacube_utilities.createAOI import create_lat_lon\n",
    "import glob\n",
    "import datacube\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "dc = datacube.Datacube(app='wofs dask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client\n",
    "client = Client('dask-scheduler.dask.svc.cluster.local:8786')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aoi_wkt = \"POLYGON((178.12 -18.25,178.15 -18.25,178.15 -18.27,178.12 -18.27,178.12 -18.25))\"\n",
    "\n",
    "#aoi_wkt = \"POLYGON((178.67520332337 -18.046588897702, 178.68601799012 -18.046588897702, 178.68610382081 -18.057231903073, 178.67537498475 -18.05740356445, 178.67520332337 -18.046588897702))\"\n",
    "aoi_wkt = \"POLYGON((178.68713378906 -17.997493743896, 178.69846343994 -18.00624847412, 178.68301391601 -18.024959564208, 178.67288589477 -18.017063140868, 178.67305755615 -18.017063140868, 178.68713378906 -17.997493743896))\"\n",
    "\n",
    "time_range = (\"2000\", \"2019\")\n",
    "\n",
    "tide_range = (0.00, 1.5)\n",
    "\n",
    "time_step = '1Y'\n",
    "\n",
    "#output_projection = \"EPSG:32760\"\n",
    "output_projection = \"EPSG:3460\"\n",
    "crs = \"EPSG:3460\"\n",
    "\n",
    "res = (30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tide_files = glob.glob(\"/home/jovyan/odc-hub/tides/*.csv\")\n",
    "print(tide_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_extents, lon_extents = create_lat_lon(aoi_wkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterline_funcs.display_map(latitude=lat_extents, longitude=lon_extents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Proj, transform\n",
    "inProj  = Proj(\"+init=EPSG:4326\")\n",
    "outProj = Proj(\"+init=EPSG:3460\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lat, max_lat = (lat_extents) \n",
    "min_lon, max_lon = (lon_extents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x_A, y_A = transform(inProj, outProj, min_lon, min_lat)\n",
    "x_B, y_B = transform(inProj, outProj, max_lon, max_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_range = (y_A, y_B)\n",
    "lon_range = (x_A, x_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create resolution\n",
    "resolution = (-res, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_chunks = dict(\n",
    "    x = 1000,\n",
    "    y = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Landsat-8 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'query' dictionary object, which contains the longitudes, latitudes and time provided above\n",
    "query = {\n",
    "    'y': lat_range,\n",
    "    'x': lon_range,\n",
    "    'time': time_range,\n",
    "    'output_crs': output_projection,  \n",
    "    'resolution': resolution,\n",
    "    'dask_chunks': dask_chunks,\n",
    "    'crs': crs,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Import L8 scenes for the given period\n",
    "scenes = dc.load(product=\"ls8_water_classification\",\n",
    "                 group_by='solar_day',\n",
    "                 measurements = ['water_classification'],\n",
    "                 **query\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the 'query' dictionary object, which contains the longitudes, \n",
    "# latitudes and time provided above\n",
    "query = {\n",
    "    'y': lat_extents,\n",
    "    'x': lon_extents,\n",
    "    'time': time_range,\n",
    "    'resolution': resolution,\n",
    "    'output_crs': output_projection,  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ard(dc,\n",
    "             products=None,\n",
    "             min_gooddata=0.0,\n",
    "             fmask_gooddata=[1, 4, 5],\n",
    "             mask_pixel_quality=True,\n",
    "             mask_invalid_data=True,\n",
    "             mask_contiguity='nbart_contiguity',\n",
    "             mask_dtype=np.float32,\n",
    "             ls7_slc_off=True,\n",
    "             product_metadata=False,\n",
    "             **dcload_kwargs):\n",
    "    '''\n",
    "    Loads Landsat Collection 3 or Sentinel 2 Definitive and Near Real \n",
    "    Time data for multiple sensors (i.e. ls5t, ls7e and ls8c for \n",
    "    Landsat; s2a and s2b for Sentinel 2), and returns a single masked \n",
    "    xarray dataset containing only observations that contain greater \n",
    "    than a given proportion of good quality pixels. This can be used \n",
    "    to extract clean time series of observations that are not affected \n",
    "    by cloud, for example as an input to the `animated_timeseries` \n",
    "    function from `dea_plotting`.\n",
    "    \n",
    "    The proportion of good quality pixels is calculated by summing the \n",
    "    pixels flagged as good quality in `fmask`. By default non-cloudy or \n",
    "    shadowed land, snow and water pixels are treated as good quality, \n",
    "    but this can be customised using the `fmask_gooddata` parameter.\n",
    "    \n",
    "    Last modified: February 2020\n",
    "    \n",
    "    Parameters\n",
    "    ----------  \n",
    "    dc : datacube Datacube object\n",
    "        The Datacube to connect to, i.e. `dc = datacube.Datacube()`.\n",
    "        This allows you to also use development datacubes if required.    \n",
    "    products : list\n",
    "        A list of product names to load data from. Valid options are \n",
    "        ['ga_ls5t_ard_3', 'ga_ls7e_ard_3', 'ga_ls8c_ard_3'] for Landsat,\n",
    "        ['s2a_ard_granule', 's2b_ard_granule'] for Sentinel 2 Definitive, \n",
    "        and ['s2a_nrt_granule', 's2b_nrt_granule'] for Sentinel 2 Near \n",
    "        Real Time (on the DEA Sandbox only).\n",
    "    min_gooddata : float, optional\n",
    "        An optional float giving the minimum percentage of good quality \n",
    "        pixels required for a satellite observation to be loaded. \n",
    "        Defaults to 0.0 which will return all observations regardless of\n",
    "        pixel quality (set to e.g. 0.99 to return only observations with\n",
    "        more than 99% good quality pixels).\n",
    "    fmask_gooddata : list, optional\n",
    "        An optional list of fmask values to treat as good quality \n",
    "        observations in the above `min_gooddata` calculation. The \n",
    "        default is `[1, 4, 5]` which will return non-cloudy or shadowed \n",
    "        land, snow and water pixels. Choose from: \n",
    "        `{'0': 'nodata', '1': 'valid', '2': 'cloud', \n",
    "          '3': 'shadow', '4': 'snow', '5': 'water'}`.\n",
    "    mask_pixel_quality : bool, optional\n",
    "        An optional boolean indicating whether to apply the good data \n",
    "        mask to all observations that were not filtered out for having \n",
    "        less good quality pixels than `min_gooddata`. E.g. if \n",
    "        `min_gooddata=0.99`, the filtered observations may still contain \n",
    "        up to 1% poor quality pixels. The default of False simply \n",
    "        returns the resulting observations without masking out these \n",
    "        pixels; True masks them and sets them to NaN using the good data \n",
    "        mask. This will convert numeric values to floating point values \n",
    "        which can cause memory issues, set to False to prevent this.\n",
    "    mask_invalid_data : bool, optional\n",
    "        An optional boolean indicating whether invalid -999 nodata \n",
    "        values should be replaced with NaN. These invalid values can be\n",
    "        caused by missing data along the edges of scenes, or terrain \n",
    "        effects (for NBART). Be aware that masking out invalid values \n",
    "        will convert all numeric values to floating point values when \n",
    "        -999 values are replaced with NaN, which can cause memory issues.\n",
    "    mask_contiguity : str or bool, optional\n",
    "        An optional string or boolean indicating whether to mask out \n",
    "        pixels missing data in any band (i.e. \"non-contiguous\" values). \n",
    "        Although most missing data issues are resolved by \n",
    "        `mask_invalid_data`, this step is important for generating \n",
    "        clean and concistent composite datasets. The default\n",
    "        is `mask_contiguity='nbart_contiguity'` which will set any \n",
    "        pixels with non-contiguous values to NaN based on NBART data. \n",
    "        If you are loading NBAR data instead, you should specify\n",
    "        `mask_contiguity='nbar_contiguity'` instead. To ignore non-\n",
    "        contiguous values completely, set `mask_contiguity=False`.\n",
    "        Be aware that masking out non-contiguous values will convert \n",
    "        all numeric values to floating point values when -999 values \n",
    "        are replaced with NaN, which can cause memory issues.\n",
    "    mask_dtype : numpy dtype, optional\n",
    "        An optional parameter that controls the data type/dtype that\n",
    "        layers are coerced to when when `mask_pixel_quality=True` or \n",
    "        `mask_contiguity=True`. Defaults to `np.float32`, which uses\n",
    "        approximately 1/2 the memory of `np.float64`.\n",
    "    ls7_slc_off : bool, optional\n",
    "        An optional boolean indicating whether to include data from \n",
    "        after the Landsat 7 SLC failure (i.e. SLC-off). Defaults to \n",
    "        True, which keeps all Landsat 7 observations > May 31 2003. \n",
    "    product_metadata : bool, optional\n",
    "        An optional boolean indicating whether to return the dataset \n",
    "        with a `product` variable that gives the name of the product \n",
    "        that each observation in the time series came from (e.g. \n",
    "        'ga_ls5t_ard_3'). Defaults to False.\n",
    "    **dcload_kwargs : \n",
    "        A set of keyword arguments to `dc.load` that define the \n",
    "        spatiotemporal query used to extract data. This typically\n",
    "        includes `measurements`, `x`, `y`, `time`, `resolution`, \n",
    "        `resampling`, `group_by` and `crs`. Keyword arguments can \n",
    "        either be listed directly in the `load_ard` call like any \n",
    "        other parameter (e.g. `measurements=['nbart_red']`), or by \n",
    "        passing in a query kwarg dictionary (e.g. `**query`). For a \n",
    "        list of possible options, see the `dc.load` documentation: \n",
    "        https://datacube-core.readthedocs.io/en/latest/dev/api/generate/datacube.Datacube.load.html          \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    combined_ds : xarray Dataset\n",
    "        An xarray dataset containing only satellite observations that \n",
    "        contains greater than `min_gooddata` proportion of good quality \n",
    "        pixels.   \n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Due to possible bug in xarray 0.13.0, define temporary function \n",
    "    # which converts dtypes in a way that preserves attributes\n",
    "    def astype_attrs(da, dtype=np.float32):\n",
    "        '''\n",
    "        Loop through all data variables in the dataset, record \n",
    "        attributes, convert to a custom dtype, then reassign attributes. \n",
    "        If the data variable cannot be converted to the custom dtype \n",
    "        (e.g. trying to convert non-numeric dtype like strings to \n",
    "        floats), skip and return the variable unchanged.\n",
    "        \n",
    "        This can be combined with `.where()` to save memory. By casting \n",
    "        to e.g. np.float32, we prevent `.where()` from automatically \n",
    "        casting to np.float64, using 2x the memory. np.float16 could be \n",
    "        used to save even more memory (although this may not be \n",
    "        compatible with all downstream applications).\n",
    "        \n",
    "        This custom function is required instead of using xarray's \n",
    "        built-in `.astype()`, due to a bug in xarray 0.13.0 that drops\n",
    "        attributes: https://github.com/pydata/xarray/issues/3348\n",
    "        '''\n",
    "        \n",
    "        try:            \n",
    "            da_attr = da.attrs\n",
    "            da = da.astype(dtype)\n",
    "            da = da.assign_attrs(**da_attr)\n",
    "            return da\n",
    "        \n",
    "        except ValueError:        \n",
    "            return da         \n",
    "    dcload_kwargs = deepcopy(dcload_kwargs)  \n",
    "    \n",
    "    # Determine if lazy loading is required\n",
    "    lazy_load = 'dask_chunks' in dcload_kwargs\n",
    "    \n",
    "    # Warn user if they combine lazy load with min_gooddata\n",
    "    if (min_gooddata > 0.0) & lazy_load:\n",
    "                warnings.warn(\"Setting 'min_gooddata' percentage to > 0.0 \"\n",
    "                              \"will cause dask arrays to compute when \"\n",
    "                              \"loading pixel-quality data to calculate \"\n",
    "                              \"'good pixel' percentage. This can \"\n",
    "                              \"significantly slow the return of your dataset.\")\n",
    "    \n",
    "    # Verify that products were provided, and that only Sentinel-2 or \n",
    "    # only Landsat products are being loaded at the same time\n",
    "    if not products:\n",
    "        raise ValueError(\"Please provide a list of product names \"\n",
    "                         \"to load data from. Valid options are: \\n\"\n",
    "                         \"['ga_ls5t_ard_3', 'ga_ls7e_ard_3', 'ga_ls8c_ard_3'] \" \n",
    "                         \"for Landsat, ['s2a_ard_granule', \"\n",
    "                         \"'s2b_ard_granule'] \\nfor Sentinel 2 Definitive, or \"\n",
    "                         \"['s2a_nrt_granule', 's2b_nrt_granule'] for \"\n",
    "                         \"Sentinel 2 Near Real Time\")\n",
    "    elif all(['ls' in product for product in products]):\n",
    "        product_type = 'ls'\n",
    "    elif all(['s2' in product for product in products]):\n",
    "        product_type = 's2'\n",
    "    else:\n",
    "        raise ValueError(\"Loading both Sentinel-2 and Landsat data \"\n",
    "                         \"at the same time is currently not supported\")\n",
    "\n",
    "    # Create a list to hold data for each product\n",
    "    product_data = []\n",
    "\n",
    "    # Iterate through each requested product\n",
    "    for product in products:\n",
    "\n",
    "        try:\n",
    "\n",
    "            # Load data including fmask band\n",
    "            print(f'Loading {product} data')\n",
    "            try:\n",
    "                \n",
    "                # If dask_chunks is specified, load data using query\n",
    "                if lazy_load:\n",
    "                    ds = dc.load(product=f'{product}',\n",
    "                                 **dcload_kwargs)\n",
    "                \n",
    "                # If no dask chunks specified, add this param so that\n",
    "                # we can lazy load data before filtering by good data\n",
    "                else:\n",
    "                    ds = dc.load(product=f'{product}',\n",
    "                                 dask_chunks={},\n",
    "                                 **dcload_kwargs) \n",
    "                \n",
    "            except KeyError as e:\n",
    "                raise ValueError(f'Band {e} does not exist in this product. '\n",
    "                                 f'Verify all requested `measurements` exist '\n",
    "                                 f'in {products}')\n",
    "            \n",
    "            # Keep a record of the original number of observations\n",
    "            total_obs = len(ds.time)\n",
    "            print(total_obs)\n",
    "            \n",
    "            # Identify all pixels not affected by cloud/shadow/invalid\n",
    "            good_quality = ds\n",
    "                \n",
    "            # If any data was returned\n",
    "            if len(ds.time) > 0:\n",
    "\n",
    "                # Optionally apply pixel quality mask to observations \n",
    "                # remaining after the filtering step above to mask out \n",
    "                # all remaining bad quality pixels\n",
    "                if mask_pixel_quality:\n",
    "                    print('    Applying pixel quality/cloud mask')\n",
    "\n",
    "                    # Change dtype to custom float before masking to \n",
    "                    # save memory. See `astype_attrs` func docstring \n",
    "                    # above for details  \n",
    "                    ds = ds.apply(astype_attrs, \n",
    "                                  dtype=mask_dtype, \n",
    "                                  keep_attrs=True)\n",
    "                    ds = ds.where(good_quality)\n",
    "                    \n",
    "                # Optionally filter to replace no data values with nans\n",
    "                if mask_invalid_data:\n",
    "                    print('    Applying invalid data mask')\n",
    "\n",
    "                    # Change dtype to custom float before masking to \n",
    "                    # save memory. See `astype_attrs` func docstring \n",
    "                    # above for details           \n",
    "                    ds = ds.apply(astype_attrs, \n",
    "                                  dtype=mask_dtype, \n",
    "                                  keep_attrs=True)\n",
    "                    ds = masking.mask_invalid_data(ds)\n",
    "\n",
    "                # If any data was returned, add result to list\n",
    "                product_data.append(ds)\n",
    "               \n",
    "            # If no data is returned, print status\n",
    "            else:\n",
    "                print(f'    No data for {product}')\n",
    "                \n",
    "\n",
    "            # If  AttributeError due to there being no variables in\n",
    "            # the dataset, skip this product and move on to the next\n",
    "        except AttributeError:\n",
    "            print(f'    No data for {product}')\n",
    "               # If any data was returned above, combine into one xarray\n",
    "    if (len(product_data) > 0):\n",
    "        # Concatenate results and sort by time\n",
    "        print(f'Combining and sorting data')\n",
    "        combined_ds = xr.concat(product_data, dim='time').sortby('time')\n",
    "        \n",
    "        # If `lazy_load` is True, return data as a dask array without\n",
    "        # actually loading it in\n",
    "        if lazy_load:\n",
    "            print(f'    Returning {len(combined_ds.time)} observations'' as a dask array')\n",
    "            return combined_ds\n",
    "        else:\n",
    "            print(f'    Returning {len(combined_ds.time)} observations ')\n",
    "            return combined_ds.compute()\n",
    "\n",
    "            # If no data was returned:\n",
    "    else:\n",
    "        print('No data returned for query')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the tide data. Edit this to make sure you have data that overlaps with what you are working on.\n",
    "# the data here comes from http://www.bom.gov.au/oceanography/projects/spslcmp/data/index.shtml for fiji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load available data from all three Landsat satellites\n",
    "#from datacube_utilities.dea_datahandling import load_ard\n",
    "landsat_ds = load_ard(dc=dc,\n",
    "                      products=['ls8_water_classification', \n",
    "                                'ls7_water_classification',\n",
    "                                'ls5_water_classification',\n",
    "                                'ls4_water_classification'],\n",
    "                      group_by='solar_day',\n",
    "                      mask_invalid_data=False,\n",
    "                      mask_pixel_quality=False,\n",
    "                      **query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#water_classes.isel(time = 4).water_classification.plot();\n",
    "water_classes = dask.delayed(landsat_ds.where(landsat_ds >= 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_classes_comp = water_classes.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Do we need to use dask for this?\n",
    "dfs = []\n",
    "# the data here comes from http://www.bom.gov.au/oceanography/projects/spslcmp/data/index.shtml for fij\n",
    "for f in tide_files:\n",
    "\n",
    "\n",
    "    # We must read the data now because it doesn't exist on the dask workers\n",
    "    df = pd.read_csv(f)\n",
    "    print(df.shape)\n",
    "    if 'Sea Level' in df.columns: \n",
    "        df['tides'] = df['Sea Level']\n",
    "        df = pd.DataFrame.drop(df, columns=['Sea Level'])\n",
    "    elif 'tide' in df.columns:\n",
    "        df['tides'] = df['tide']\n",
    "        df = pd.DataFrame.drop(df, columns=['tide'])\n",
    "    else: \n",
    "        print('variables correct')\n",
    "        #df2 = df\n",
    "    if ' Date & UTC Time' in df.columns:\n",
    "        df['time'] = df[' Date & UTC Time']\n",
    "        df = pd.DataFrame.drop(df, columns=[' Date & UTC Time'])\n",
    "    else:\n",
    "        print('variable2 correct')\n",
    "    #print(df['time'])\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "tide_data = pd.concat(dfs)\n",
    "print(tide_data.shape)    \n",
    "tide_data['time'] = pd.to_datetime(tide_data['time'], infer_datetime_format=False)\n",
    "print(tide_data.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tide_data.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import tide height data \n",
    "Into pandas and concatenate each year together.\n",
    "\n",
    "Shoreline location varies with tides, only tide heights at specific conditions are kept - determined by tide_range setting. If tide_range values at 0.00, 2.00 then only tides between 0 and 2m relative to Mean Sea Level are kept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tide_data['tide_height'] = tide_data['tides']\n",
    "new = tide_data.set_index('time')\n",
    "df4 = new.loc[~new.index.duplicated(keep='first')]\n",
    "print(df4.shape)\n",
    "df5 = df4[df4.tides != -9999]\n",
    "print(df5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we convert the data to an xarray dataset so we can analyse it in the same way as our Landsat data\n",
    "tide_data_xr = df5.to_xarray()\n",
    "# We want to convert our hourly tide heights to estimates of exactly how high the tide was at the time that\n",
    "# each satellite image was taken. To do this, we can use `.interp` to 'interpolate' a tide height for each\n",
    "# Landsat timestamp:\n",
    "landsat_tideheights = tide_data_xr.interp(time=water_classes_comp.time)\n",
    "\n",
    "# We then want to put these values back into the Landsat dataset so that each image has an estimated tide height:\n",
    "water_classes_comp['tide_height'] = landsat_tideheights.tide_height\n",
    "print((water_classes_comp))\n",
    "# Plot the resulting tide heights for each Landsat image:\n",
    "water_classes_comp.tide_height.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Landsat Images by tide height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep the Landsat Images which correspond to the desired tide height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat_hightide = water_classes_comp.where((water_classes_comp.tide_height > tide_range[0]) & \n",
    "                                   (water_classes_comp.tide_height < tide_range[1]), drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the resulting tide heights for each Landsat image:\n",
    "landsat_hightide.tide_height.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Summary Images of Water."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat_resampled = landsat_hightide.water.compute().resample(time=time_step).mean('time')\n",
    "landsat_resampled.plot(col='time', cmap='RdBu', col_wrap=3, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Shorelines from Imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up attributes to assign to each waterline\n",
    "attribute_data = {'time': [str(i)[0:10] for i in landsat_resampled.time.values]}\n",
    "attribute_dtypes = {'time': 'str'}\n",
    "\n",
    "contour_gdf = waterline_funcs.contour_extract(\n",
    "    z_values=[0],\n",
    "    ds_array=landsat_resampled,\n",
    "    ds_crs=landsat_ds.crs,\n",
    "    ds_affine=landsat_ds.geobox.transform,\n",
    "    output_shp=f'output_waterlines.shp',\n",
    "    attribute_data=attribute_data,\n",
    "    attribute_dtypes=attribute_dtypes,\n",
    "    min_vertices=5\n",
    ")\n",
    "\n",
    "# Plot output shapefile over the top of the first year's MNDWI layer\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 7))\n",
    "landsat_resampled.isel(time=-1).plot(ax=ax, cmap='Greys', alpha=1.0, edgecolors=None)\n",
    "contour_gdf.plot(cmap='ocean', ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('broken')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Interactive map of output shorelines coloured by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterline_funcs.map_shapefile(gdf=contour_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterline_funcs.display_map(latitude=lat_extents, longitude=lon_extents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cubeenv]",
   "language": "python",
   "name": "conda-env-cubeenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
